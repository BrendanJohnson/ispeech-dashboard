# iSpeech Dashboard

## :bulb: Getting Started

1. Type `npm install` in the source folder where `package.json` is located
2. Type `npm run dev` to start the local development server
3. If this doesn't work might need to downgrade CoreJS using `npm install --save core-js@2`

The repo uses [vue-cli](https://github.com/vuejs/vue-cli) scaffolding which takes care of the development setup with webpack and all the necessary modern tools to make web development faster and easier.

## :hammer: Build

### Build for production with minification
`npm run build`
### Run unit tests
`npm run unit`
### Run and watch unit tests
`npm run unit:watch`

## :rocket: Deployment

### Submit a build to Google Cloud
`gcloud builds submit --tag gcr.io/ispeech-brendan/ispeech-dashboard`

### Deploy build to Google Cloud Run
`gcloud run deploy --image gcr.io/ispeech-brendan/ispeech-dashboard --platform managed --region us-central1 --memory 256Mi`



## :book: How to Use

### Processing Audio 
To process audio click on the "New Session" option in the main menu:
![alt text](https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/new_session.jpg?raw=true)
Then selecting the language and the model type to use for speaker type analysis.

Note that by default "Custom Model" is selected which uses audio collected from the uploaded for the current Child to predict whether the child is talking or not. The audio data used to build to the Custom Model can be seen in the Custom Model panel below. If processing audio for a new child where no data is currently available to perform speaker analysis, it is advised to use to change the Custom Model setting to Standard Model.

### Analyzing Speech 
To view the analysis data for sessions that have already been processed, click on the "Sessions" option in the main menu. To find the session either scroll through the list of sessions or use the Search box at the top of the screen to search session by name, date, or transcript content. Once a session has been found click on the session name to expand the details:
![alt text](https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/analysis_session.jpg?raw=true)

#### Audio Timeline
Once the session has been expanded it is possible to view the audio timeline of the session:  

<img src="https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/timeline.jpg?raw=true" alt="Audio timeline" align="center" width="70%">

In the timeline, audio segments that have been identified as child speech are coloured in pink, and adult speech identified as blue. You can click on any part of the timeline to play back the audio for that section, and skip to that part of the transcript. Alternatively, the Play/Pause button can be used to listen to the entire audio sample.

#### Transcript
The auto-generated transcript is shown in the panel under the timeline. Each line contains the time period that an utterance was detected, the duration of the utterance, the name of the detected speaker (either the child's name or the teacher's name) and the transcript text.

<img src="https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/transcript_line.jpg?raw=true" alt="Transcript line" align="center" width="80%">

To generate an NLP analysis chart of the text in the transcript, click on the "?" button. If the transcript generated by Speech-To-Text is not correct, it is possible to modify the transcript line manually by clicking the button in the middle to type a new transcript, or to use dictation to re-input the transcript using the microphone button on the right.

#### NLP
After clicking the "?" button, the NLP analysis chart will be displayed under the transcript line:  

<img src="https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/NLP.jpg?raw=true" alt="NLP line" align="center" width="80%">

In the NLP chart the relationship between the parts of speech are connected, with the word type (Noun, Verb, Pronoun etc.) displayed underneath. To the left of the NLP chart is an emoticon displaying the overrall emotion of the utterance, for example:

| Emoticon | Sentiment |
| -------- | --------- |
| <img src="https://github.com/iSpeechAPAC/ispeech-dashboard/raw/master/emoji-laughing.jpg" alt="Laughing Emoticon" align="center" width="25"> | Very Positive |
| <img src="https://github.com/iSpeechAPAC/ispeech-dashboard/raw/master/emoji-smile.jpg" alt="Smile Emoticon" align="center" width="25"> | Positive      |
| <img src="https://github.com/iSpeechAPAC/ispeech-dashboard/raw/master/emoji-neutral.jpg" alt="Neutral Emoticon" align="center" width="25"> | Neutral       |
| <img src="https://github.com/iSpeechAPAC/ispeech-dashboard/raw/master/emoji-frown.jpg" alt="Frown Emoticon" align="center" width="25"> | Negative       |
| <img src="https://github.com/iSpeechAPAC/ispeech-dashboard/raw/master/emoji-dizzy.jpg" alt="Dizzy Emoticon" align="center" width="25"> | Very Negative       |

### Statistics
To view overall statistics for the child, click on the "Dashboard" item in the main menu:
![alt text](https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/dashboard_overview.jpg?raw=true)

On the Dashboard, you can see the overall statistics for the selected child, such as the number of Sessions taken, the total amount of minutes of child speech detected accross all sessions and the number of words detected. On the right side of the screen recommendations for the child are shown based on their current progress.

#### Number of utterances
The No. of Utterances panel displays the evolution of utterances made by the child and adult over time:  

<img src="https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/dashboard_utterances.jpg?raw=true" alt="Utterances Panel" align="center" width="400">

Generally a low number of child utterances during the session indicates a low amount of engagement in the session, although this may depend on session type.

#### Child/Adult ratio
The Child/Adult ratio panel shows the amount of child speech over time across sessions:  

<img src="https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/dashboard_adult_child.jpg?raw=true" alt="Child/Adult ratio Panel" align="center" width="400">

This is another indicator of the child's engagement in the session, however the metric may be more accurate than the number of utterances in the case where the long utterances are expected (for example, during storytelling sessions)

#### Language Spoken
The Language Spoken panel displays the composition of languages spoken during sessions, based on the amount of minutes spoken in each language over all sessions:  

<img src="https://github.com//iSpeechAPAC/ispeech-dashboard/blob/master/dashboard_languages.jpg?raw=true" alt="Language Spoken Panel" align="center" width="200">

Currently only Cantonese, Mandarin and English are supported.

## :cloud: Cloud configuration

### Cloud Bucket configuration
In order to access XML manifests stored in Google Cloud, CORS needs to be setup on the GCS bucket.
After authentication with your Google Account, use gsutil to execute the following command with the provided cors.json:
`gsutil cors set cors.json gs://ispeech-manifests`

### Installation of gcloud CLI
Normally the gcloud tool should be installed using the instructions at https://cloud.google.com/sdk/docs/quickstart.

On MacOS there maybe an error after trying to run the init.sh script due to the installed version of the openssl library being wrong (usually too new). The following commands helped to fix the issue:   

1. Install an older version of openssl using Homebrew:
```
brew uninstall openssl
brew tap-new $USER/old-openssl
brew extract --version=1.0.2t openssl $USER/old-openssl
brew install openssl@1.0.2t
```
Note that the first time I ran this there was an error due to needing to access Github to download the version, I remedied this by first running `export HOMEBREW_NO_GITHUB_API=1` which sets an environment variable.

2. Create a symlink to point to the Tap installed by Homebrew: `ln -s /usr/local/opt/openssl@1.0.2t /usr/local/opt/openssl`

